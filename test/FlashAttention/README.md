# FlashAttention Testing and Profiling

This directory contains scripts for benchmarking and profiling FlashAttention (Prefill) and FlashDecoding (Decode) performance using the analytical model and real GPU execution.

## 1. Running Performance Tests (`test_perf.py`)

The `test_perf.py` script runs performance tests across various configurations (Batch Size, Sequence Length, Attention Heads, etc.) for defined models. It compares the analytical model's estimated latency against a Roofline model and real GPU measurements.

### Usage

```bash
python test_perf.py [OPTIONS]
```

### Arguments

- `--device`: Target hardware device. Choices: `Orin`, `Thor`. Default: `Orin`.
- `--mode`: Attention mode. Choices: `prefill`, `decode`. Default: `prefill`.
- `--model`: Target LLM model configuration. Choices: `InternVision`, `Qwen3_0_6B`, `Qwen3_1_7B`, `Qwen3_4B`, `Qwen3_8B`, `all`. Default: `all`.
- `--precision`: Data precision. Choices: `fp32`, `fp16`, `int8`, `int4`, `all`. Default: `all`.
- `--update`: Which latency metrics to update/run. Choices: `ours` (Analytical Model), `roofline` (Roofline Model), `gpu` (Real GPU), `all`. Default: `all`.

### Examples

**Run prefill tests for all models in fp16 on Orin:**
```bash
python test_perf.py --device Orin --mode prefill --precision fp16
```

**Run decode tests for Qwen3_7B only, updating only the analytical model results:**
```bash
python test_perf.py --mode decode --model Qwen3_1_7B --update ours
```

**Output:**
The results are saved in `results_perf/<model>/<precision>/<mode>/attention_TP.csv`.

---

## 2. plotting Results (`plot_attention.py`)

The `plot_attention.py` script reads the `attention_TP.csv` file generated by `test_perf.py` and generates plots comparing the `Ours` (Analytical), `Roofline`, and `GPU` latencies.

### Usage

```bash
python plot_attention.py [input_directory]
```

### Arguments

- `input_directory`: (Optional) The directory containing the `attention_TP.csv` file. Defaults to the current directory (`.`).

### Examples

**Plot results from the current directory:**
```bash
python plot_attention.py
```

**Plot results from a specific result folder:**
```bash
python plot_attention.py results_perf/Qwen3_1_7B/fp16/prefill/
```

### Output

- The script detects whether the data is for **Prefill** (X-axis: Sequence Length) or **Decoding** (X-axis: KV Cache Length).
- Plots are saved in a `plots/` subdirectory within the input directory.
- Plot files are named based on the configuration (e.g., `prefill_bs1_nhq32_nhkv8_hd128.png`).

## Notes

- **Profiling GPU:** Running with `--update gpu` requires a CUDA-enabled GPU and appropriate dependencies (PyTorch, FlashAttention installed).
- **Analytical Model:** The analytical model simulation (`--update ours`) does not require a GPU but relies on `scalesim` and other internal modules.
